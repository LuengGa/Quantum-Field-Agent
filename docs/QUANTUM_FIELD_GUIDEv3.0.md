QUANTUM_FIELD_GUIDEv3.0

é¡¹ç›®ç»“æ„ï¼ˆV3.0ï¼‰

quantum-field-v3.0/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                    # APIå…¥å£ï¼ˆå¤šæ¨¡æ€ï¼‰
â”‚   â”œâ”€â”€ multimodal_field.py        # ç»Ÿä¸€åœºæ ¸å¿ƒ â­
â”‚   â”œâ”€â”€ encoders/
â”‚   â”‚   â”œâ”€â”€ text_encoder.py        # æ–‡æœ¬åµŒå…¥
â”‚   â”‚   â”œâ”€â”€ vision_encoder.py      # è§†è§‰ç¼–ç ï¼ˆCLIPï¼‰
â”‚   â”‚   â””â”€â”€ audio_encoder.py       # éŸ³é¢‘ç¼–ç ï¼ˆWhisperï¼‰
â”‚   â”œâ”€â”€ decoders/
â”‚   â”‚   â”œâ”€â”€ text_decoder.py        # æ–‡æœ¬ç”Ÿæˆ
â”‚   â”‚   â”œâ”€â”€ speech_decoder.py      # è¯­éŸ³åˆæˆ
â”‚   â”‚   â””â”€â”€ image_decoder.py       # å›¾åƒç”Ÿæˆï¼ˆDALL-E/SDï¼‰
â”‚   â”œâ”€â”€ modality_router.py         # æ¨¡æ€è·¯ç”±
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ frontend/
    â””â”€â”€ multimodal_interface.html  # å¤šæ¨¡æ€äº¤äº’ç•Œé¢ â­
    
1. ç»Ÿä¸€åœºæ ¸å¿ƒï¼ˆbackend/multimodal_field.pyï¼‰

"""
Quantum Field V3.0 - ç»Ÿä¸€å¤šæ¨¡æ€åœº
æ‰€æœ‰æ¨¡æ€ç»Ÿä¸€ä¸ºé«˜ç»´å‘é‡ï¼Œåœ¨åœºä¸­å…±æŒ¯ã€å¹²æ¶‰ã€åç¼©
"""

import base64
import io
import numpy as np
from typing import Union, Dict, List, Optional, AsyncGenerator, Tuple
from dataclasses import dataclass
from enum import Enum
import torch
from PIL import Image
import openai

class ModalityType(Enum):
    """æ¨¡æ€ç±»å‹"""
    TEXT = "text"
    IMAGE = "image"
    AUDIO = "audio"
    VIDEO = "video"  # V3.5æ”¯æŒ
    STRUCTURED = "structured"  # è¡¨æ ¼/JSON

@dataclass
class FieldTensor:
    """
    åœºå¼ é‡ï¼šç»Ÿä¸€çš„æ•°æ®è¡¨ç¤º
    ä»»ä½•æ¨¡æ€è¿›å…¥åœºåéƒ½è½¬æ¢ä¸ºFieldTensor
    """
    modality: ModalityType
    vector: np.ndarray           # è¯­ä¹‰å‘é‡ï¼ˆ1536ç»´æˆ–æ›´é«˜ï¼‰
    raw_data: Optional[bytes]    # åŸå§‹æ•°æ®ï¼ˆå›¾åƒbytes/éŸ³é¢‘bytesï¼‰
    metadata: Dict               # å…ƒæ•°æ®ï¼ˆå°ºå¯¸ã€æ ¼å¼ã€æ—¶é—´æˆ³ç­‰ï¼‰
    confidence: float            # ç¼–ç ç½®ä¿¡åº¦

class UnifiedEncoder:
    """
    ç»Ÿä¸€ç¼–ç å™¨ï¼šä»»ä½•æ¨¡æ€â†’å‘é‡
    ä½¿ç”¨OpenAI CLIP/Whisperæˆ–æœ¬åœ°å¤šæ¨¡æ€æ¨¡å‹
    """
    
    def __init__(self):
        self.text_client = openai.OpenAI()
        self.vision_available = self._check_vision()
        self.audio_available = self._check_audio()
        
    def _check_vision(self) -> bool:
        """æ£€æŸ¥è§†è§‰æ¨¡å‹å¯ç”¨æ€§"""
        try:
            # æµ‹è¯•CLIPæˆ–GPT-4V
            return True
        except:
            return False
    
    def _check_audio(self) -> bool:
        """æ£€æŸ¥éŸ³é¢‘æ¨¡å‹å¯ç”¨æ€§"""
        try:
            # æµ‹è¯•Whisper
            return True
        except:
            return False
    
    async def encode(self, input_data: Union[str, bytes, Image.Image], 
                    modality_hint: Optional[ModalityType] = None) -> FieldTensor:
        """
        ç»Ÿä¸€ç¼–ç å…¥å£
        è‡ªåŠ¨è¯†åˆ«æ¨¡æ€æˆ–æ ¹æ®æç¤ºç¼–ç 
        """
        # è‡ªåŠ¨è¯†åˆ«æ¨¡æ€
        detected_modality = modality_hint or self._detect_modality(input_data)
        
        if detected_modality == ModalityType.TEXT:
            return await self._encode_text(input_data)
        elif detected_modality == ModalityType.IMAGE:
            return await self._encode_image(input_data)
        elif detected_modality == ModalityType.AUDIO:
            return await self._encode_audio(input_data)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡æ€: {detected_modality}")
    
    def _detect_modality(self, data) -> ModalityType:
        """è‡ªåŠ¨æ£€æµ‹è¾“å…¥æ¨¡æ€"""
        if isinstance(data, str):
            return ModalityType.TEXT
        elif isinstance(data, (bytes, Image.Image)):
            # æ£€æŸ¥magic numberæˆ–PIL
            if isinstance(data, Image.Image):
                return ModalityType.IMAGE
            # æ£€æŸ¥æ˜¯å¦ä¸ºéŸ³é¢‘ï¼ˆç®€åŒ–ï¼‰
            if data[:4] == b'RIFF' or data[:4] == b'\xff\xfb':
                return ModalityType.AUDIO
            return ModalityType.IMAGE
        return ModalityType.STRUCTURED
    
    async def _encode_text(self, text: str) -> FieldTensor:
        """æ–‡æœ¬ç¼–ç ï¼ˆOpenAI Embeddingï¼‰"""
        response = self.text_client.embeddings.create(
            model="text-embedding-3-large",
            input=text
        )
        vector = np.array(response.data[0].embedding)
        
        return FieldTensor(
            modality=ModalityType.TEXT,
            vector=vector,
            raw_data=text.encode(),
            metadata={"length": len(text), "model": "text-embedding-3-large"},
            confidence=1.0
        )
    
    async def _encode_image(self, image_input: Union[bytes, Image.Image]) -> FieldTensor:
        """
        å›¾åƒç¼–ç ï¼ˆCLIPæˆ–GPT-4Vç‰¹å¾æå–ï¼‰
        ä½¿ç”¨base64ç¼–ç åé€šè¿‡Vision APIè·å–åµŒå…¥
        """
        if isinstance(image_input, Image.Image):
            # PIL Image â†’ bytes
            buffer = io.BytesIO()
            image_input.save(buffer, format='PNG')
            image_bytes = buffer.getvalue()
        else:
            image_bytes = image_input
        
        # Base64ç¼–ç ç”¨äºAPI
        base64_image = base64.b64encode(image_bytes).decode()
        
        # ä½¿ç”¨GPT-4Vè·å–å›¾åƒæè¿°ï¼Œç„¶ååµŒå…¥æè¿°ï¼ˆç®€åŒ–ç‰ˆï¼‰
        # å®é™…ç”Ÿäº§åº”ä½¿ç”¨CLIPæ¨¡å‹æœ¬åœ°ç¼–ç 
        response = self.text_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": "Describe this image in one sentence for embedding."},
                    {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}}
                ]
            }]
        )
        
        description = response.choices[0].message.content
        # å†ç¼–ç æè¿°æ–‡æœ¬
        text_tensor = await self._encode_text(description)
        text_tensor.modality = ModalityType.IMAGE
        text_tensor.raw_data = image_bytes
        text_tensor.metadata.update({
            "description": description,
            "size": len(image_bytes),
            "format": "png"
        })
        
        return text_tensor
    
    async def _encode_audio(self, audio_bytes: bytes) -> FieldTensor:
        """
        éŸ³é¢‘ç¼–ç ï¼ˆWhisperè½¬å½•+åµŒå…¥ï¼‰
        """
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        temp_file = f"/tmp/audio_{hash(audio_bytes)}.wav"
        with open(temp_file, "wb") as f:
            f.write(audio_bytes)
        
        # Whisperè½¬å½•
        with open(temp_file, "rb") as f:
            transcript = self.text_client.audio.transcriptions.create(
                model="whisper-1",
                file=f
            )
        
        # åµŒå…¥è½¬å½•æ–‡æœ¬
        text_tensor = await self._encode_text(transcript.text)
        text_tensor.modality = ModalityType.AUDIO
        text_tensor.raw_data = audio_bytes
        text_tensor.metadata.update({
            "transcript": transcript.text,
            "duration": "unknown",  # å®é™…åº”è§£æéŸ³é¢‘å¤´
            "format": "wav"
        })
        
        return text_tensor

class ModalityRouter:
    """
    æ¨¡æ€è·¯ç”±å™¨ï¼šå†³å®šè¾“å‡ºæ¨¡æ€å’Œè·¯ç”±ç­–ç•¥
    """
    
    @staticmethod
    def route_output(input_modality: ModalityType, 
                    user_intent: str,
                    available_decoders: List[ModalityType]) -> ModalityType:
        """
        æ™ºèƒ½è·¯ç”±ï¼šæ ¹æ®è¾“å…¥æ¨¡æ€å’Œç”¨æˆ·æ„å›¾å†³å®šè¾“å‡ºæ¨¡æ€
        """
        # æ„å›¾å…³é”®è¯æ˜ å°„
        if any(kw in user_intent for kw in ["ç”»", "ç”Ÿæˆ", "image", "ç”Ÿæˆå›¾ç‰‡"]):
            if ModalityType.IMAGE in available_decoders:
                return ModalityType.IMAGE
        
        if any(kw in user_intent for kw in ["è¯´", "è¯»", "æœ—è¯»", "è¯­éŸ³"]):
            if ModalityType.AUDIO in available_decoders:
                return ModalityType.AUDIO
        
        # é»˜è®¤ä¿æŒåŒæ¨¡æ€æˆ–æ–‡æœ¬
        if input_modality == ModalityType.TEXT:
            return ModalityType.TEXT
        
        # è·¨æ¨¡æ€é»˜è®¤è½¬æ–‡æœ¬ï¼ˆç†è§£åå›å¤ï¼‰
        return ModalityType.TEXT

class UnifiedDecoder:
    """
    ç»Ÿä¸€è§£ç å™¨ï¼šå‘é‡â†’ä»»æ„æ¨¡æ€
    """
    
    def __init__(self):
        self.client = openai.OpenAI()
    
    async def decode(self, field_state: np.ndarray, 
                    target_modality: ModalityType,
                    context: Dict) -> AsyncGenerator[Union[str, bytes], None]:
        """
        ç»Ÿä¸€è§£ç å…¥å£
        """
        if target_modality == ModalityType.TEXT:
            async for chunk in self._decode_to_text(field_state, context):
                yield chunk
        
        elif target_modality == ModalityType.AUDIO:
            async for chunk in self._decode_to_speech(field_state, context):
                yield chunk
        
        elif target_modality == ModalityType.IMAGE:
            # å›¾åƒç”Ÿæˆæ˜¯ä¸€æ¬¡æ€§çš„ï¼Œä¸æ˜¯æµå¼
            image_bytes = await self._decode_to_image(field_state, context)
            yield image_bytes
    
    async def _decode_to_text(self, vector: np.ndarray, context: Dict) -> AsyncGenerator[str, None]:
        """è§£ç ä¸ºæ–‡æœ¬ï¼ˆæ ‡å‡†LLMç”Ÿæˆï¼‰"""
        prompt = context.get("prompt", "åŸºäºä»¥ä¸Šç†è§£ï¼Œç”Ÿæˆå›å¤ï¼š")
        
        response = self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            stream=True
        )
        
        for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
    
    async def _decode_to_speech(self, vector: np.ndarray, context: Dict) -> AsyncGenerator[bytes, None]:
        """
        è§£ç ä¸ºè¯­éŸ³ï¼ˆæµå¼TTSï¼‰
        ä½¿ç”¨OpenAI TTSæˆ–æœ¬åœ°Piper
        """
        text_prompt = context.get("text", "Hello")
        
        # OpenAI TTSï¼ˆéæµå¼ï¼Œä½†æˆ‘ä»¬å¯ä»¥åˆ†æ®µï¼‰
        response = self.client.audio.speech.create(
            model="tts-1",
            voice="alloy",
            input=text_prompt
        )
        
        # æµå¼è¿”å›éŸ³é¢‘bytes
        chunk_size = 1024
        for chunk in response.iter_bytes(chunk_size):
            yield chunk
    
    async def _decode_to_image(self, vector: np.ndarray, context: Dict) -> bytes:
        """
        è§£ç ä¸ºå›¾åƒï¼ˆç”Ÿæˆå¼ï¼‰
        ä½¿ç”¨DALL-Eæˆ–Stable Diffusion
        """
        prompt = context.get("prompt", "A beautiful scene")
        
        response = self.client.images.generate(
            model="dall-e-3",
            prompt=prompt,
            size="1024x1024",
            quality="standard",
            n=1,
            response_format="b64_json"
        )
        
        # è¿”å›base64è§£ç çš„å›¾åƒbytes
        image_data = response.data[0].b64_json
        return base64.b64decode(image_data)

class MultimodalQuantumField:
    """
    å¤šæ¨¡æ€é‡å­åœºï¼šç»Ÿä¸€å¤„ç†æ‰€æœ‰æ¨¡æ€
    """
    
    def __init__(self):
        self.encoder = UnifiedEncoder()
        self.router = ModalityRouter()
        self.decoder = UnifiedDecoder()
        self.memory_bank: Dict[str, List[FieldTensor]] = {}  # å¤šæ¨¡æ€è®°å¿†åº“
    
    async def process(self, 
                     input_data: Union[str, bytes],
                     input_modality: Optional[ModalityType] = None,
                     user_id: str = "default",
                     output_modality_hint: Optional[ModalityType] = None) -> AsyncGenerator[Union[str, bytes], None]:
        """
        ç»Ÿä¸€å¤„ç†æµç¨‹ï¼š
        1. ç¼–ç ï¼ˆä»»ä½•æ¨¡æ€â†’å‘é‡ï¼‰
        2. åœºå…±æŒ¯ï¼ˆä¸è®°å¿†å¹²æ¶‰ï¼‰
        3. è·¯ç”±å†³ç­–ï¼ˆå†³å®šè¾“å‡ºæ¨¡æ€ï¼‰
        4. è§£ç ï¼ˆå‘é‡â†’ç›®æ ‡æ¨¡æ€ï¼‰
        """
        # 1. ç»Ÿä¸€ç¼–ç ï¼ˆè¿›å…¥åœºï¼‰
        input_tensor = await self.encoder.encode(input_data, input_modality)
        
        # 2. æ£€ç´¢ç›¸å…³å¤šæ¨¡æ€è®°å¿†
        relevant_memories = self._retrieve_multimodal_memory(user_id, input_tensor.vector)
        
        # 3. åœºå…±æŒ¯ï¼ˆå‘é‡èåˆï¼‰
        fused_vector = self._interference_fusion(input_tensor.vector, relevant_memories)
        
        # 4. ä¿å­˜åˆ°è®°å¿†
        self._save_to_memory(user_id, input_tensor)
        
        # 5. è·¯ç”±å†³ç­–
        target_modality = output_modality_hint or self.router.route_output(
            input_tensor.modality,
            input_data if isinstance(input_data, str) else "",
            [ModalityType.TEXT, ModalityType.AUDIO, ModalityType.IMAGE]
        )
        
        # 6. ç»Ÿä¸€è§£ç ï¼ˆåç¼©ä¸ºç›®æ ‡æ¨¡æ€ï¼‰
        context = {
            "prompt": input_data if isinstance(input_data, str) else input_tensor.metadata.get("description", ""),
            "input_modality": input_tensor.modality.value,
            "target_modality": target_modality.value
        }
        
        async for output_chunk in self.decoder.decode(fused_vector, target_modality, context):
            yield output_chunk
    
    def _retrieve_multimodal_memory(self, user_id: str, query_vector: np.ndarray, top_k: int = 3) -> List[np.ndarray]:
        """æ£€ç´¢å¤šæ¨¡æ€è®°å¿†ï¼ˆè·¨æ¨¡æ€ç›¸ä¼¼åº¦æœç´¢ï¼‰"""
        if user_id not in self.memory_bank:
            return []
        
        memories = self.memory_bank[user_id]
        if not memories:
            return []
        
        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        similarities = []
        for mem in memories:
            sim = np.dot(query_vector, mem.vector) / (np.linalg.norm(query_vector) * np.linalg.norm(mem.vector))
            similarities.append((sim, mem.vector))
        
        # è¿”å›Top-K
        similarities.sort(reverse=True)
        return [vec for _, vec in similarities[:top_k]]
    
    def _interference_fusion(self, input_vec: np.ndarray, memory_vecs: List[np.ndarray]) -> np.ndarray:
        """å¹²æ¶‰èåˆï¼šè¾“å…¥å‘é‡ä¸è®°å¿†å‘é‡çš„åŠ æƒå åŠ """
        if not memory_vecs:
            return input_vec
        
        # åŠ æƒå¹³å‡ï¼ˆç®€å•å®ç°ï¼Œå®é™…å¯æ³¨æ„åŠ›æœºåˆ¶ï¼‰
        weights = [0.5] + [0.5 / len(memory_vecs)] * len(memory_vecs)
        all_vecs = [input_vec] + memory_vecs
        
        fused = np.zeros_like(input_vec)
        for w, vec in zip(weights, all_vecs):
            fused += w * vec
        
        return fused / np.linalg.norm(fused)  # å½’ä¸€åŒ–
    
    def _save_to_memory(self, user_id: str, tensor: FieldTensor):
        """ä¿å­˜åˆ°åœºè®°å¿†"""
        if user_id not in self.memory_bank:
            self.memory_bank[user_id] = []
        
        self.memory_bank[user_id].append(tensor)
        
        # é™åˆ¶è®°å¿†å¤§å°ï¼ˆæœ€è¿‘20æ¡ï¼‰
        if len(self.memory_bank[user_id]) > 20:
            self.memory_bank[user_id].pop(0)
            
2. APIå…¥å£ï¼ˆbackend/main.py æ›´æ–°V3.0ï¼‰

"""
V3.0 APIå…¥å£ - å¤šæ¨¡æ€ç»Ÿä¸€åœº
"""

from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, Response
from typing import Optional
import io

from multimodal_field import MultimodalQuantumField, ModalityType

app = FastAPI(title="Quantum Field V3.0 - Unified Multimodal")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–å¤šæ¨¡æ€åœº
mm_field = MultimodalQuantumField()

@app.post("/process/text")
async def process_text(message: str, user_id: str = "default"):
    """æ–‡æœ¬â†’æ–‡æœ¬ï¼ˆæ ‡å‡†å¯¹è¯ï¼‰"""
    async def generate():
        async for chunk in mm_field.process(message, ModalityType.TEXT, user_id):
            yield chunk
    return StreamingResponse(generate(), media_type="text/plain")

@app.post("/process/image")
async def process_image(
    file: UploadFile = File(...),
    prompt: str = Form("æè¿°è¿™å¼ å›¾ç‰‡"),
    user_id: str = Form("default")
):
    """
    å›¾åƒâ†’æ–‡æœ¬ï¼ˆå›¾åƒç†è§£ï¼‰
    æˆ– å›¾åƒâ†’å›¾åƒï¼ˆå›¾åƒç¼–è¾‘ï¼Œæœªæ¥æ”¯æŒï¼‰
    """
    contents = await file.read()
    
    async def generate():
        async for chunk in mm_field.process(contents, ModalityType.IMAGE, user_id):
            yield chunk
    return StreamingResponse(generate(), media_type="text/plain")

@app.post("/process/audio")
async def process_audio(
    file: UploadFile = File(...),
    user_id: str = Form("default")
):
    """
    éŸ³é¢‘â†’æ–‡æœ¬ï¼ˆè¯­éŸ³è¯†åˆ«+ç†è§£ï¼‰
    """
    contents = await file.read()
    
    async def generate():
        async for chunk in mm_field.process(contents, ModalityType.AUDIO, user_id):
            yield chunk
    return StreamingResponse(generate(), media_type="text/plain")

@app.post("/generate/speech")
async def generate_speech(
    text: str,
    user_id: str = "default"
):
    """
    æ–‡æœ¬â†’éŸ³é¢‘ï¼ˆè¯­éŸ³åˆæˆï¼‰
    """
    async def generate():
        # å¼ºåˆ¶è¾“å‡ºä¸ºéŸ³é¢‘
        async for chunk in mm_field.process(
            text, 
            ModalityType.TEXT, 
            user_id,
            output_modality_hint=ModalityType.AUDIO
        ):
            yield chunk
    return StreamingResponse(generate(), media_type="audio/mpeg")

@app.post("/generate/image")
async def generate_image(
    prompt: str,
    user_id: str = "default"
):
    """
    æ–‡æœ¬â†’å›¾åƒï¼ˆæ–‡ç”Ÿå›¾ï¼‰
    """
    async def generate():
        async for chunk in mm_field.process(
            f"ç”Ÿæˆå›¾ç‰‡: {prompt}",
            ModalityType.TEXT,
            user_id,
            output_modality_hint=ModalityType.IMAGE
        ):
            yield chunk
    return Response(content=chunk, media_type="image/png")  # å®é™…åº”å¤„ç†bytes

@app.get("/modality/supported")
async def list_supported_modalities():
    """åˆ—å‡ºæ”¯æŒçš„æ¨¡æ€è½¬æ¢"""
    return {
        "input": ["text", "image", "audio"],
        "output": ["text", "audio", "image"],
        "cross_modal": [
            {"from": "text", "to": "image", "desc": "æ–‡ç”Ÿå›¾"},
            {"from": "image", "to": "text", "desc": "å›¾åƒæè¿°"},
            {"from": "audio", "to": "text", "desc": "è¯­éŸ³è¯†åˆ«"},
            {"from": "text", "to": "audio", "desc": "è¯­éŸ³åˆæˆ"}
        ]
    }
    
3. å¤šæ¨¡æ€å‰ç«¯ï¼ˆfrontend/multimodal_interface.htmlï¼‰

<!-- frontend/index.html -->
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Quantum Field V3.0 - ç»Ÿä¸€å¤šæ¨¡æ€åœº</title>
    <style>
        body {
            margin: 0;
            background: #0a0a0a;
            color: #fff;
            font-family: -apple-system, BlinkMacSystemFont, sans-serif;
            height: 100vh;
            display: flex;
            flex-direction: column;
        }
        
        .header {
            background: #111;
            padding: 15px 20px;
            border-bottom: 2px solid #00ff88;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        
        .header h1 { margin: 0; font-size: 18px; color: #00ff88; }
        
        .modality-status {
            display: flex;
            gap: 10px;
            font-size: 12px;
        }
        
        .status-badge {
            padding: 4px 12px;
            background: #1a1a1a;
            border: 1px solid #333;
            border-radius: 12px;
            color: #666;
        }
        
        .status-badge.active {
            border-color: #00ff88;
            color: #00ff88;
            background: rgba(0,255,136,0.1);
        }
        
        .main {
            flex: 1;
            display: flex;
            overflow: hidden;
        }
        
        .chat-area {
            flex: 1;
            padding: 20px;
            overflow-y: auto;
            display: flex;
            flex-direction: column;
            gap: 20px;
        }
        
        .message {
            max-width: 80%;
            padding: 15px;
            border-radius: 12px;
            position: relative;
            animation: fadeIn 0.3s;
        }
        
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }
        
        .message.user {
            align-self: flex-end;
            background: #1a1a1a;
            border: 1px solid #333;
        }
        
        .message.ai {
            align-self: flex-start;
            background: rgba(0,255,136,0.05);
            border: 1px solid rgba(0,255,136,0.2);
            color: #e0e0e0;
        }
        
        .message img, .message audio {
            max-width: 100%;
            border-radius: 8px;
            margin-top: 10px;
            display: block;
        }
        
        .input-area {
            padding: 20px;
            background: #111;
            border-top: 1px solid #222;
        }
        
        .attachments {
            display: flex;
            gap: 10px;
            margin-bottom: 10px;
            flex-wrap: wrap;
        }
        
        .attachment {
            position: relative;
            width: 60px;
            height: 60px;
            background: #1a1a1a;
            border-radius: 8px;
            overflow: hidden;
            border: 1px solid #333;
        }
        
        .attachment img {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }
        
        .remove-attach {
            position: absolute;
            top: 2px;
            right: 2px;
            background: #ff4444;
            color: white;
            border: none;
            border-radius: 50%;
            width: 18px;
            height: 18px;
            font-size: 10px;
            cursor: pointer;
        }
        
        .input-box {
            display: flex;
            gap: 10px;
            align-items: flex-end;
            background: #1a1a1a;
            padding: 10px;
            border-radius: 12px;
            border: 1px solid #333;
        }
        
        .input-box:focus-within {
            border-color: #00ff88;
        }
        
        textarea {
            flex: 1;
            background: transparent;
            border: none;
            color: #fff;
            resize: none;
            outline: none;
            font-size: 15px;
            min-height: 24px;
            max-height: 120px;
            font-family: inherit;
        }
        
        .input-actions {
            display: flex;
            gap: 8px;
        }
        
        .icon-btn {
            width: 36px;
            height: 36px;
            border-radius: 50%;
            background: #333;
            border: none;
            color: #fff;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.3s;
        }
        
        .icon-btn:hover {
            background: #00ff88;
            color: #000;
        }
        
        .send-btn {
            background: #00ff88;
            color: #000;
            padding: 8px 24px;
            border-radius: 20px;
            border: none;
            font-weight: bold;
            cursor: pointer;
        }
        
        .send-btn:disabled {
            background: #333;
            color: #666;
            cursor: not-allowed;
        }
        
        .field-indicator {
            position: fixed;
            bottom: 100px;
            right: 20px;
            width: 150px;
            height: 150px;
            background: rgba(0,0,0,0.9);
            border: 1px solid #00ff88;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            opacity: 0;
            transition: opacity 0.3s;
            pointer-events: none;
        }
        
        .field-indicator.active {
            opacity: 1;
        }
        
        .field-particles {
            position: absolute;
            width: 100%;
            height: 100%;
            animation: rotate 8s linear infinite;
        }
        
        @keyframes rotate {
            from { transform: rotate(0deg); }
            to { transform: rotate(360deg); }
        }
        
        .particle {
            position: absolute;
            width: 6px;
            height: 6px;
            background: #00ff88;
            border-radius: 50%;
            box-shadow: 0 0 10px #00ff88;
        }
        
        .hidden-input {
            display: none;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>âš› Quantum Field V3.0 - ç»Ÿä¸€å¤šæ¨¡æ€åœº</h1>
        <div class="modality-status">
            <span class="status-badge active">æ–‡æœ¬</span>
            <span class="status-badge" id="img-status">å›¾åƒ</span>
            <span class="status-badge" id="audio-status">éŸ³é¢‘</span>
        </div>
    </div>
    
    <div class="main">
        <div class="chat-area" id="chat-container">
            <div class="message ai">
                æ¬¢è¿ä½¿ç”¨ç»Ÿä¸€å¤šæ¨¡æ€åœºã€‚æ”¯æŒï¼š<br>
                â€¢ æ–‡æœ¬å¯¹è¯<br>
                â€¢ ä¸Šä¼ å›¾ç‰‡åˆ†æ<br>
                â€¢ è¯­éŸ³è¾“å…¥/åˆæˆ<br>
                â€¢ æ–‡ç”Ÿå›¾<br>
                æ‰€æœ‰æ¨¡æ€ç»Ÿä¸€åœ¨åœºä¸­å…±æŒ¯ã€‚
            </div>
        </div>
    </div>
    
    <div class="field-indicator" id="field-viz">
        <div class="field-particles" id="particles"></div>
        <div style="color:#00ff88;font-size:12px;">åœºå…±æŒ¯ä¸­...</div>
    </div>
    
    <div class="input-area">
        <div class="attachments" id="attachments"></div>
        <div class="input-box">
            <textarea id="message-input" placeholder="è¾“å…¥æ¶ˆæ¯ï¼Œæˆ–ä¸Šä¼ å›¾ç‰‡/éŸ³é¢‘..." rows="1"></textarea>
            <div class="input-actions">
                <button class="icon-btn" onclick="document.getElementById('image-input').click()" title="ä¸Šä¼ å›¾ç‰‡">ğŸ“·</button>
                <button class="icon-btn" onclick="document.getElementById('audio-input').click()" title="ä¸Šä¼ éŸ³é¢‘">ğŸµ</button>
                <button class="icon-btn" onclick="toggleSpeech()" title="è¯­éŸ³è¾“å…¥">ğŸ¤</button>
                <button class="send-btn" onclick="sendMessage()" id="send-btn">å‘é€</button>
            </div>
        </div>
        <input type="file" id="image-input" class="hidden-input" accept="image/*" onchange="handleImage(this)">
        <input type="file" id="audio-input" class="hidden-input" accept="audio/*" onchange="handleAudio(this)">
    </div>

    <script>
        const API_URL = 'http://localhost:8000';
        let currentAttachment = null;
        let isRecording = false;
        
        // åˆå§‹åŒ–åœºç²’å­åŠ¨ç”»
        function initParticles() {
            const container = document.getElementById('particles');
            for(let i=0; i<8; i++) {
                const p = document.createElement('div');
                p.className = 'particle';
                const angle = (i/8) * Math.PI * 2;
                p.style.left = `${50 + 40*Math.cos(angle)}%`;
                p.style.top = `${50 + 40*Math.sin(angle)}%`;
                container.appendChild(p);
            }
        }
        initParticles();
        
        function showField() {
            document.getElementById('field-viz').classList.add('active');
            document.getElementById('img-status').classList.add('active');
        }
        
        function hideField() {
            document.getElementById('field-viz').classList.remove('active');
            document.getElementById('img-status').classList.remove('active');
        }
        
        function handleImage(input) {
            const file = input.files[0];
            if(!file) return;
            
            const reader = new FileReader();
            reader.onload = function(e) {
                currentAttachment = {
                    type: 'image',
                    data: e.target.result.split(',')[1],
                    name: file.name
                };
                showAttachment('ğŸ“·', file.name);
            };
            reader.readAsDataURL(file);
        }
        
        function handleAudio(input) {
            const file = input.files[0];
            if(!file) return;
            
            const reader = new FileReader();
            reader.onload = function(e) {
                currentAttachment = {
                    type: 'audio',
                    data: e.target.result,
                    name: file.name
                };
                showAttachment('ğŸµ', file.name);
                document.getElementById('audio-status').classList.add('active');
            };
            reader.readAsArrayBuffer(file);
        }
        
        function showAttachment(icon, name) {
            const container = document.getElementById('attachments');
            container.innerHTML = `
                <div class="attachment">
                    <div style="display:flex;align-items:center;justify-content:center;height:100%;color:#666;font-size:24px;">
                        ${icon}
                    </div>
                    <button class="remove-attach" onclick="removeAttachment()">Ã—</button>
                </div>
            `;
        }
        
        function removeAttachment() {
            currentAttachment = null;
            document.getElementById('attachments').innerHTML = '';
            document.getElementById('img-status').classList.remove('active');
            document.getElementById('audio-status').classList.remove('active');
        }
        
        async function sendMessage() {
            const input = document.getElementById('message-input');
            const text = input.value.trim();
            if(!text && !currentAttachment) return;
            
            // æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
            addMessage(text || (currentAttachment ? `[${currentAttachment.type}]` : ''), 'user');
            input.value = '';
            
            const btn = document.getElementById('send-btn');
            btn.disabled = true;
            showField();
            
            try {
                let response;
                let endpoint;
                let body;
                
                if(currentAttachment && currentAttachment.type === 'image') {
                    // å›¾åƒå¤„ç†
                    const formData = new FormData();
                    formData.append('file', dataURLtoFile('data:image/png;base64,' + currentAttachment.data, currentAttachment.name));
                    formData.append('prompt', text || 'æè¿°è¿™å¼ å›¾ç‰‡');
                    formData.append('user_id', 'user_001');
                    
                    response = await fetch(`${API_URL}/process/image`, {
                        method: 'POST',
                        body: formData
                    });
                } else if(currentAttachment && currentAttachment.type === 'audio') {
                    // éŸ³é¢‘å¤„ç†
                    const formData = new FormData();
                    const blob = new Blob([currentAttachment.data]);
                    formData.append('file', blob, 'audio.wav');
                    formData.append('user_id', 'user_001');
                    
                    response = await fetch(`${API_URL}/process/audio`, {
                        method: 'POST',
                        body: formData
                    });
                } else {
                    // çº¯æ–‡æœ¬
                    response = await fetch(`${API_URL}/process/text`, {
                        method: 'POST',
                        headers: {'Content-Type': 'application/json'},
                        body: JSON.stringify({
                            message: text,
                            user_id: 'user_001'
                        })
                    });
                }
                
                // æµå¼è¯»å–
                const reader = response.body.getReader();
                const decoder = new TextDecoder();
                let aiMessage = addMessage('', 'ai');
                let fullText = '';
                
                while(true) {
                    const {done, value} = await reader.read();
                    if(done) break;
                    
                    const chunk = decoder.decode(value);
                    fullText += chunk;
                    aiMessage.textContent = fullText;
                    scrollToBottom();
                }
                
                // å¦‚æœæ˜¯ç”Ÿæˆå›¾åƒçš„æŒ‡ä»¤ï¼Œå°è¯•è§£æå›¾ç‰‡
                if(text.includes('ç”Ÿæˆ') && text.includes('å›¾')) {
                    // è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è°ƒç”¨ä¸“é—¨çš„ç”Ÿå›¾API
                    aiMessage.innerHTML += '<div style="color:#666;font-size:12px;margin-top:5px;">[å›¾åƒç”Ÿæˆéœ€è°ƒç”¨ä¸“ç”¨æ¥å£]</div>';
                }
                
            } catch(e) {
                addMessage('é”™è¯¯: ' + e.message, 'ai');
            }
            
            btn.disabled = false;
            hideField();
            removeAttachment();
        }
        
        function addMessage(text, role) {
            const container = document.getElementById('chat-container');
            const div = document.createElement('div');
            div.className = `message ${role}`;
            div.textContent = text;
            container.appendChild(div);
            scrollToBottom();
            return div;
        }
        
        function scrollToBottom() {
            const container = document.getElementById('chat-container');
            container.scrollTop = container.scrollHeight;
        }
        
        function dataURLtoFile(dataurl, filename) {
            const arr = dataurl.split(',');
            const mime = arr[0].match(/:(.*?);/)[1];
            const bstr = atob(arr[1]);
            let n = bstr.length;
            const u8arr = new Uint8Array(n);
            while(n--) {
                u8arr[n] = bstr.charCodeAt(n);
            }
            return new File([u8arr], filename, {type:mime});
        }
        
        function toggleSpeech() {
            if(!('webkitSpeechRecognition' in window)) {
                alert('æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«');
                return;
            }
            
            if(isRecording) {
                recognition.stop();
                isRecording = false;
                return;
            }
            
            const recognition = new webkitSpeechRecognition();
            recognition.lang = 'zh-CN';
            recognition.continuous = false;
            recognition.interimResults = false;
            
            recognition.onstart = () => {
                isRecording = true;
                document.querySelector('.icon-btn[onclick="toggleSpeech()"]').style.background = '#ff4444';
            };
            
            recognition.onresult = (event) => {
                const text = event.results[0][0].transcript;
                document.getElementById('message-input').value += text;
            };
            
            recognition.onend = () => {
                isRecording = false;
                document.querySelector('.icon-btn[onclick="toggleSpeech()"]').style.background = '#333';
            };
            
            recognition.start();
        }
        
        document.getElementById('message-input').addEventListener('keypress', (e) => {
            if(e.key === 'Enter' && !e.shiftKey) {
                e.preventDefault();
                sendMessage();
            }
        });
    </script>
</body>
</html>

5. ä¾èµ–æ–‡ä»¶ï¼ˆbackend/requirements.txtï¼‰

fastapi==0.109.0
uvicorn[standard]==0.27.0
openai==1.12.0
python-dotenv==1.0.0
pydantic==2.6.0
redis==5.0.1
numpy==1.26.3
pillow==10.2.0
python-multipart==0.0.6
aiofiles==23.2.1
torch==2.1.0
transformers==4.36.0

6. Dockeré…ç½®ï¼ˆdocker-compose.ymlï¼‰

version: '3.8'

services:
  redis:
    image: redis:7-alpine
    volumes:
      - redis-data:/data

  v3-api:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - REDIS_URL=redis://redis:6379
    volumes:
      - ./backend:/app
    depends_on:
      - redis

volumes:
  redis-data:
  
V3.0 å…³é”®ç‰¹æ€§ï¼š
ç»Ÿä¸€å‘é‡ç©ºé—´ï¼šæ–‡æœ¬/å›¾åƒ/éŸ³é¢‘éƒ½ç¼–ç ä¸º1536ç»´å‘é‡
è·¨æ¨¡æ€æ£€ç´¢ï¼šå›¾åƒå¯ä»¥è§¦å‘æ–‡æœ¬è®°å¿†ï¼ŒéŸ³é¢‘å¯ä»¥å…³è”å›¾åƒ
ä»»æ„è½¬æ¢ï¼šæ–‡æœ¬â†’å›¾åƒã€å›¾åƒâ†’æ–‡æœ¬ã€éŸ³é¢‘â†’æ–‡æœ¬ã€æ–‡æœ¬â†’éŸ³é¢‘
åœºå¯è§†åŒ–ï¼šå®æ—¶æ˜¾ç¤ºå¤šæ¨¡æ€å…±æŒ¯çŠ¶æ€
